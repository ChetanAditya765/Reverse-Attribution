# Reverse Attribution (RA) â€” Text & Vision

Reverse Attribution (RA) highlights **counterâ€‘evidence**â€”the parts of an input that *suppress* the predicted class and would flip the decision if removed.

This repo includes a polished Streamlit demo, a clickâ€‘toâ€‘mask interaction for **text & vision**, Captum baselines, and scripts to train/evaluate and reproduce RA analysis figures.

Youtube Demo : https://youtu.be/vf-oaVzn1iQ
---

## âœ¨ Whatâ€™s inside

* **Unified demo**: Text (BERT on IMDB, RoBERTa on Yelp) & Vision (ResNet56 on CIFARâ€‘10, tiny CNN fallback).
* **Reverse Attribution** with runnerâ€‘up contrast and **Aâ€‘Flip** score.
* **Clickâ€‘toâ€‘mask** tokens/patches â†’ instant Î”prob and Aâ€‘Flip recompute.
* **Baselines**: Captum LayerIntegratedGradients / IntegratedGradients.
* **Evaluate**: Saved metrics panel + quick compute (accuracy / CE loss).
* **Reproduction**: scripts to aggregate results and render figures.

---

## ğŸ—‚ Repo layout (key parts)

```
Reverse-Attribution/
â”œâ”€â”€ app.py                         # Streamlit app
â”œâ”€â”€ ra/                            # RA implementation & visualizer glue
â”œâ”€â”€ models/                        # BERT/RoBERTa wrappers, ResNet56 or tiny CNN
â”œâ”€â”€ scripts/                       # train/eval driver(s)
â”œâ”€â”€ reproduce_results.py           # optional analysis aggregator
â”œâ”€â”€ visualizer.py                  # figure renderer for aggregated results
â”œâ”€â”€ checkpoints/                   # put your local weights here (not tracked)
â”‚   â”œâ”€â”€ bert_imdb/best_model.pt
â”‚   â”œâ”€â”€ roberta_yelp/best_model.pt
â”‚   â””â”€â”€ resnet56_cifar10/best_model.pt
â”œâ”€â”€ reproduction_results/          # optional CSV/JSON output (tables/plots)
â”‚   â”œâ”€â”€ table1_performance.csv
â”‚   â”œâ”€â”€ table2_ra_analysis.csv
â”‚   â””â”€â”€ table3_model_integration.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ RA_NCNTAIA2025.pdf             # paper (preprint)
```

> **Note:** Weights are largeâ€”keep them out of git. See **Checkpoints** below.

---

## ğŸ”§ Environment

<details>
<summary><strong>Windows (Anaconda Prompt / conda)</strong></summary>

```bat
cd C:\Users\cheta\OneDrive\Documents\GitHub\Reverse-Attribution

conda create -n ra-env python=3.9 -y
conda activate ra-env

:: if you have a setup.py/pyproject this enables editable mode
pip install -e .

:: pinned torch/torchtext combo known to work well
pip install torch==2.1.0 torchtext==0.16.0

:: huggingface + streaming datasets + fsspec for local/remote files
pip install -U "datasets>=2.19.1" "huggingface_hub>=0.21.2" "fsspec>=2023.12.0"

pip install ipython
pip install -r requirements.txt  :: (if present; safe to re-run)

:: Optional (helps avoid tokenizer warnings):
setx TOKENIZERS_PARALLELISM False
```

</details>

<details>
<summary><strong>Linux / macOS</strong></summary>

```bash
# (optional) conda environment
conda create -n ra-env python=3.9 -y
conda activate ra-env

# editable install if you have setup.py/pyproject
pip install -e .

# pinned torch/torchtext combo known to work well
pip install torch==2.1.0 torchtext==0.16.0

# huggingface + streaming datasets + fsspec for local/remote files
pip install -U "datasets>=2.19.1" "huggingface_hub>=0.21.2" "fsspec>=2023.12.0"

pip install ipython
pip install -r requirements.txt  # (if present; safe to re-run)

# Optional (helps avoid tokenizer warnings)
export TOKENIZERS_PARALLELISM=False
```

> Paths use `/` on Unix; drop `setx` and use `export` for env vars.

</details>

---

## ğŸ’¾ Checkpoints (local only)

Create folders and drop your fineâ€‘tuned weights as `best_model.pt`:

Hugging Space Repo : https://huggingface.co/KillBill765/Reverse-Attribution
```
checkpoints/
â”œâ”€â”€ bert_imdb/best_model.pt
â”œâ”€â”€ roberta_yelp/best_model.pt
â””â”€â”€ resnet56_cifar10/best_model.pt
```

* If a weight file is missing, **text models** fall back to HF pretrained; **vision** falls back to a small CNN so the demo still runs.
* **Do not push** `.pt` files to GitHub. Use local paths, cloud storage, or HF Hub for sharing.

---

## ğŸƒ Training & Evaluation (scripts)

From the project root with `ra-env` activated:

```bat
:: Text model training (e.g., BERT on IMDB)
python scripts/script.py --stage train --model_type text

:: Vision model training (e.g., ResNet56/TinyCNN on CIFAR-10)
python scripts/script.py --stage train --model_type vision

:: Evaluation (produces training_history.json, CSVs, and/or JSON summary)
python scripts/script.py --stage eval
```

**Conventions (used by the app):**

Each run stores `training_history.json` under:

* `checkpoints/bert_imdb/`, `checkpoints/roberta_yelp/`, or `checkpoints/resnet56_cifar10/`.

Optional â€œcomprehensiveâ€ summary:

* `checkpoints/<subdir>/comprehensive_evaluation_results.json`, or
* `<checkpoints_root>/comprehensive_evaluation_results.json`.

**Example JSON snippet** read by the app (flexible; only keys found are shown):

```json
{
  "standard_metrics": { "avg_loss": 0.2274, "num_samples": 25000 },
  "ra_analysis": { "summary": { "avg_a_flip": 96.67 } }
}
```

---

## ğŸ› Streamlit Demo

Run the app:

```bash
streamlit run app.py
```

In the sidebar:

* Set **Checkpoints root** to your local `checkpoints` absolute path.
* Choose **Task** and **Mode**.

### Modes

**Live Demo**

* **Text**: paste a review â†’ Predict + Explain â†’ RA token table/heatmap.

  * Click to mask tokens (checkboxes) â†’ **Apply mask** â†’ see Î”prob & Aâ€‘Flip (beforeâ†’after).
* **Vision**: upload a 32Ã—32 image â†’ Predict + Explain â†’ RA heatmap overlay.

  * Click to mask 8Ã—8 patches â†’ Î”prob & Aâ€‘Flip recompute.

**RA vs Baselines**

* Sideâ€‘byâ€‘side RA vs Captum LIG/IG token/patch rankings.

**Evaluate**

* **Saved Metrics (no compute)**: reads `training_history.json` and optional `comprehensive_evaluation_results.json`, shows metric cards and raw JSON.
* **Quick Evaluation (compute)**: accuracy & average crossâ€‘entropy on IMDB/Yelp or CIFARâ€‘10.

**Reproducibility**

* Device/PyTorch info and a **Download `config.json`** button.

---

## ğŸ“œ Reproduce aggregated results & figures

```bash
python reproduce_results.py
python visualizer.py --input comprehensive_evaluation_results.json --outdir figs/
```

**Expected outputs:**

* `reproduction_results/` (CSVs like `table1_performance.csv`, `table2_ra_analysis.csv`, `table3_model_integration.csv`)
* `figs/` with plots exported by `visualizer.py`.

---

## ğŸ“ Where to put CSVs (if you already have them)

If you have preâ€‘computed tables, place them here so the app/scripts can pick them up:

```
reproduction_results/
â”œâ”€â”€ table1_performance.csv
â”œâ”€â”€ table2_ra_analysis.csv
â””â”€â”€ table3_model_integration.csv
```

---

## ğŸ§ª Tips & Troubleshooting

* **Hugging Face downloads:** the first run may download base models/tokenizers; keep internet access on.
* **CUDA:** if you donâ€™t have a matching CUDA build, run on CPU (it worksâ€”just slower).
* **Tokenizer missing:** loaders in `app.py` attach `model.tokenizer`; if you customize models, ensure a tokenizer is set.
* **Captum errors:** `pip install captum`.
* **Large weights:** donâ€™t commit them; consider Git LFS/HF Hub if you must share.


---

## ğŸ“¬ Contact

* **Author:** Chetan Aditya
* **Email** chetan.lakka@gmail.com

